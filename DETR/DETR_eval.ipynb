{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DETR - Training and Evaluation\n",
    "\n",
    "This notebook contains the code and commands to train and evaluate the results of the DETR algorithm.\n",
    "\n",
    "You can either download a [pretrained model](https://drive.google.com/file/d/1TB7vYB7PmeyE_ryDhrczXNZujgK-4uKG/view?usp=sharing) or run the following commands to train it yourself.\n",
    "\n",
    "To run the pretrained model just download the .pth file and place it in the DETR/ directory.\n",
    "\n",
    "To train the model yourself simply run the following commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command will train the DETR model with arguments set to sensible defaults. Here are some optional arguments:\n",
    "- -b\n",
    "    * Specify the batch size. The optimal value is usually around double the number of GPU cores on your machine.\n",
    "- -e\n",
    "    * Specify the number of epochs\n",
    "- --cuda\n",
    "    * Include this flag to enable training on the GPU.\n",
    "\n",
    "This may take many hours. The script will save each epoch if it performs better than the previous epoch and will save the model in the DETR directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using hardware acceleration\n",
      "/home/jamie/cs9517/HighDefinition/DETR\n",
      "Training face_detr with batch_size=8\n",
      "\n",
      "Using cache found in /home/jamie/.cache/torch/hub/facebookresearch_detr_main\n",
      "/home/jamie/cs9517/opencv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jamie/cs9517/opencv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "100%|█████████████████████████████| 63/63 [00:13<00:00,  4.51it/s, loss=1.33e+3]\n",
      "100%|███████████████████████████████| 9/9 [00:00<00:00, 10.85it/s, loss=1.33e+3]\n",
      "|EPOCH 1| TRAIN_LOSS 1332.7449951171875| VALID_LOSS 1325.9341634114583|\n",
      "Best model found in Epoch 1........Saving Model\n",
      " 51%|██████████████▋              | 32/63 [00:05<00:05,  5.47it/s, loss=1.32e+3]^C\n",
      " 51%|██████████████▋              | 32/63 [00:05<00:05,  5.34it/s, loss=1.32e+3]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jamie/cs9517/HighDefinition/DETR/train.py\", line 111, in <module>\n",
      "    model = train_detr.run(args, device, train_dataset, valid_dataset)\n",
      "  File \"/home/jamie/cs9517/HighDefinition/DETR/train_detr.py\", line 194, in run\n",
      "    train_loss = train_fn(train_dataloader, model,criterion, optimizer,device,scheduler=None,epoch=epoch)\n",
      "  File \"/home/jamie/cs9517/HighDefinition/DETR/train_detr.py\", line 138, in train_fn\n",
      "    summary_loss.update(losses.item(),BATCH_SIZE)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py -b 8 -e 500 --cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DETR Result Visualisation\n",
    "\n",
    "The following code is used to evaluate the results of a trained DETR model. It loads the trained model and a random image and displays the bounding box on the image as a sanity check. It will then compute some metrics about the model such as precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "#import pandas as pd \n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "from tqdm.autonotebook import tqdm\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "\n",
    "#Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "\n",
    "# Dataloader\n",
    "from dataset import CustomImageDataset\n",
    "\n",
    "#sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "#CV\n",
    "import cv2\n",
    "\n",
    "################# DETR FUCNTIONS FOR LOSS######################## \n",
    "import sys\n",
    "import os\n",
    "print(os.getcwd())\n",
    "# The python interpreter is dumb so we need to change directories\n",
    "sys.path.insert(0, os.getcwd() + 'facebook_DETR/')\n",
    "sys.path.insert(0, os.getcwd() + 'facebook_DETR/detr/')\n",
    "\n",
    "print(sys.path)\n",
    "\n",
    "from detr.models.matcher import HungarianMatcher\n",
    "#from detr import SetCriterion\n",
    "#################################################################\n",
    "\n",
    "#Albumenatations\n",
    "#import albumentations as A\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "#from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "#Glob\n",
    "from glob import glob\n",
    "\n",
    "from train_detr import DETRModel, collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = CustomImageDataset('../images/valid', annotation_path=\"../images/valid_annotations\", transforms=None)\n",
    "valid_dataloader = DataLoader(valid_dataset, \n",
    "                                batch_size=1, \n",
    "                                shuffle=True,\n",
    "                                collate_fn=collate_fn)\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "n_folds = 5\n",
    "seed = 42\n",
    "num_classes = 2 # Penguin or turtle\n",
    "num_queries = 1 # Only one in image but if there are multiple we can increment this\n",
    "null_class_coef = 0.5\n",
    "BATCH_SIZE = 1\n",
    "LR = 2e-5\n",
    "EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check\n",
    "\n",
    "The following code creates an instance of the DETR model and loads the weights from a filename specified in path. It will then display an image with the bounding boxes drawn on top as a sanity check. However, as the model fails to train effectively the box is 1x1 pixel and not easily visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"detr_best_456.pth\"\n",
    "\n",
    "model = DETRModel(num_classes=num_classes,num_queries=num_queries)\n",
    "model.load_state_dict(torch.load(path))\n",
    "\n",
    "images, targets = next(iter(valid_dataloader))\n",
    "\n",
    "images = list(img.to(device) for img in images)\n",
    "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "\n",
    "outputs = [{k: v.to(\"cpu\") for k, v in outputs.items()}]\n",
    "\n",
    "sample = images[0].cpu().permute(1, 2, 0).numpy()\n",
    "\n",
    "for box in outputs[0][\"pred_boxes\"][0]:\n",
    "    #print(sample)\n",
    "    box = box.numpy().astype(int)\n",
    "    # This draws the bounding box but since it has a height and width of 1 it is very hard\n",
    "    # to see\n",
    "    cv2.rectangle(sample,\n",
    "                (box[0], box[1]),\n",
    "                (box[2]+box[0], box[3]+box[1]),\n",
    "                (220, 0, 0), 1)\n",
    "\n",
    "plt.imshow(sample)\n",
    "plt.show()\n",
    "\n",
    "max_out = outputs[0][\"pred_logits\"].softmax(-1).max(-1)\n",
    "topk = max_out.values.topk(15)\n",
    "print(outputs[0][\"pred_logits\"][0, topk.indices, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "'''\n",
    "Some helper functions\n",
    "'''\n",
    "def compute_iou(x1, y1, w1, h1, x2, y2, w2, h2):\n",
    "    # Calculate the (x, y)-coordinates of the intersection rectangle\n",
    "    inter_top_left_x = max(x1, x2)\n",
    "    inter_top_left_y = max(y1, y2)\n",
    "    inter_bottom_right_x = min(x1 + w1, x2 + w2)\n",
    "    inter_bottom_right_y = min(y1 + h1, y2 + h2)\n",
    "    \n",
    "    inter_area = max(0, inter_bottom_right_x - inter_top_left_x) * max(0, inter_bottom_right_y - inter_top_left_y)\n",
    "\n",
    "    # Compute the area of both the prediction and ground-truth rectangles\n",
    "    box1_area = w1 * h1\n",
    "    box2_area = w2 * h2\n",
    "\n",
    "    # Compute the intersection over union by taking the intersection area and dividing it by the sum of prediction + ground-truth areas - the intersection area\n",
    "    iou = inter_area / float(box1_area + box2_area - inter_area)\n",
    "\n",
    "    # return the intersection over union value\n",
    "    return iou\n",
    "\n",
    "def calculate_distance(x1, y1, x2, y2):\n",
    "    return math.sqrt((x2 - x1)**2 + (y2 - y1)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_class_list = []\n",
    "output_class_list = []\n",
    "iou_list = []\n",
    "distance_list = []\n",
    "\n",
    "for image, targets in valid_dataloader:\n",
    "    images = list(img.to(device) for img in images)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "\n",
    "    target_class_list.append(targets[0][\"labels\"][0])\n",
    "    if outputs[\"pred_logits\"][0][0,0] > outputs[\"pred_logits\"][0][0,1]:\n",
    "        output_class_list.append(0)\n",
    "        print(\"found one\")\n",
    "    else:\n",
    "        output_class_list.append(1)\n",
    "\n",
    "    x1, y1, w1, h1 = targets[0][\"boxes\"][0,:].numpy()\n",
    "    x2, y2, w2, h2 = outputs[\"pred_boxes\"][0,0,:].numpy()\n",
    "    iou_list.append(compute_iou(x1, y1, w1, h1, x2, y2, w2, h2))\n",
    "    distance_list.append(calculate_distance(x1, y1, x2, y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "The following code computes some metrics about the models effectiveness. We see it almost completely fails the classification task and only receives a precision of 0.5 for the penguin class as it labels every image as a penguin.\n",
    "\n",
    "Additionally, the model always produces a bounding box at the coordinates (1, 1) with a height and width of 1 pixel. As a result the model never correctly locates the animal in each image and achieves an iou score of zero on every image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(target_class_list, output_class_list, target_names=[\"turtle\", \"penguin\"]))\n",
    "\n",
    "print(f\"iou_mean: {np.mean(iou_list)}\")\n",
    "print(f\"iou_std: {np.std(iou_list)}\")\n",
    "\n",
    "print(f\"distance mean: {np.mean(distance_list)}\")\n",
    "print(f\"distance std: {np.std(distance_list)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
